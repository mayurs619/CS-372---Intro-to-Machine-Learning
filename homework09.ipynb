{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "# COMPSCI 371 Homework 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "T"
    ]
   },
   "source": [
    "_**Group Members:**_ Mayur Sekhar, Jai Kasera, Rithvik Neti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "tags": [
     "AT"
    ]
   },
   "source": [
    "### Problem 0 (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "## Part 1: Correlation and Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 1.1 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "T"
    ]
   },
   "source": [
    "\\begin{eqnarray*}\n",
    "\\texttt{conv}(x, h, \\texttt{'full'}) &=& [6,17,8,5,18,22,4]  \\\\\n",
    "\\texttt{conv}(x, h, \\texttt{'valid'}) &=& [8,5,18] \\\\\n",
    "\\texttt{conv}(x, h, \\texttt{'same'}) &=&  [17,8,5,18,22]\\\\\n",
    "\\text{ } \\\\\n",
    "\\texttt{corr}(x, h, \\texttt{'full'}) &=& [3,16,11,4,14,24,8] \\\\\n",
    "\\texttt{corr}(x, h, \\texttt{'valid'}) &=& [11,4,14] \\\\\n",
    "\\texttt{corr}(x, h, \\texttt{'same'}) &=&  [16,11,4,14,24]\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 1.2 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "T"
    ]
   },
   "source": [
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\texttt{conv}(a, b, \\texttt{'valid'}) &=& \\begin{bmatrix}15&16\\\\31&18\\end{bmatrix}\\\\\n",
    "\\texttt{corr}(a, b, \\texttt{'valid'}) &=& \\begin{bmatrix}16&19\\\\36&28\\end{bmatrix}\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "## Part 2: Gradients of a Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 2.1 (Exam Style)\n",
    "$$C_f = \\begin{bmatrix}h_1&0&0&0&0\\\\h_2&h_1&0&0&0\\\\h_3&h_2&h_1&0&0\\\\0&h_3&h_2&h_1&0\\\\0&0&h_3&h_2&h_1\\\\0&0&0&h_3&h_2\\\\0&0&0&0&h_3\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 2.2 (Exam Style)\n",
    "$$C_v = \\begin{bmatrix}h_3&h_2&h_1&0&0\\\\0&h_3&h_2&h_1&0\\\\0&0&h_3&h_2&h_1\\end{bmatrix}$$\n",
    "\n",
    "$$C_s = \\begin{bmatrix}h_2&h_1&0&0&0\\\\h_3&h_2&h_1&0&0\\\\0&h_3&h_2&h_1&0\\\\0&0&h_3&h_2&h_1\\\\0&0&0&h_3&h_2\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 2.3 (Exam Style)\n",
    "$$ X_f = \\begin{bmatrix}x_1&0&0\\\\x_2&x_1&0\\\\x_3&x_2&x_1\\\\x_4&x_3&x_2\\\\x_5&x_4&x_3\\\\0&x_5&x_4\\\\0&0&x_5\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 2.4 (Exam Style)\n",
    "$$J_x = \\frac{\\partial y}{\\partial x} = C_f = \\begin{bmatrix}h_1&0&0&0&0\\\\h_2&h_1&0&0&0\\\\h_3&h_2&h_1&0&0\\\\0&h_3&h_2&h_1&0\\\\0&0&h_3&h_2&h_1\\\\0&0&0&h_3&h_2\\\\0&0&0&0&h_3\\end{bmatrix}$$\n",
    "\n",
    "$$J_h = \\frac{\\partial y}{\\partial h} = X_f = \\begin{bmatrix}x_1&0&0\\\\x_2&x_1&0\\\\x_3&x_2&x_1\\\\x_4&x_3&x_2\\\\x_5&x_4&x_3\\\\0&x_5&x_4\\\\0&0&x_5\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 2.5 (Exam Style)\n",
    "Given that $g_y = \\frac{\\partial l}{\\partial y}$:\n",
    "\n",
    "$$g_x = \\frac{\\partial l}{\\partial x} = C_f^{\\intercal}g_y = \\begin{bmatrix}h_1&h_2&h_3&0&0&0&0\\\\0&h_1&h_2&h_3&0&0&0\\\\0&0&h_1&h_2&h_3&0&0\\\\0&0&0&h_1&h_2&h_3&0\\\\0&0&0&0&h_1&h_2&h_3\\end{bmatrix} \\cdot \\frac{\\partial l}{\\partial y} $$\n",
    "\n",
    "$$g_h = \\frac{\\partial l}{\\partial h} = X_f^{\\intercal}g_y = \\begin{bmatrix}x_1&x_2&x_3&x_4&x_5&0&0\\\\0&x_1&x_2&x_3&x_4&x_5&0\\\\0&0&x_1&x_2&x_3&x_4&x_5\\end{bmatrix} \\cdot \\frac{\\partial l}{\\partial y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 2.6 (Exam Style)\n",
    "The operation $g_x = C_f^{\\intercal}g_y$ corresponds to the full correlation of $g_y$ with h (kernel not flipped).\n",
    "\n",
    "- Type: Full Correlation\n",
    "\n",
    "- Operands: $g_y$ and $h$\n",
    "\n",
    "- $g_x = \\texttt{conv}(g, h, \\texttt{'valid'})$\n",
    "\n",
    "The operation $g_h = X_f^{\\intercal}g_y$ corresponds to the full correlation of $g_y$ with x (kernel not flipped).\n",
    "\n",
    "- Type: Full Correlation\n",
    "\n",
    "- Operands: $g_y$ and $x$\n",
    "\n",
    "- $g_h = \\texttt{conv}(g, x, \\texttt{'valid'})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "## Part 3: A Neural Network Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fct\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import prod\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "outputs": [],
   "source": [
    "def cifar_10_loaders(data_root='./data', batch_size=4):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    train_set = torchvision.datasets.CIFAR10(\n",
    "        root=data_root, train=True, download=True, transform=transform\n",
    "    )\n",
    "    test_set = torchvision.datasets.CIFAR10(\n",
    "        root=data_root, train=False, download=True, transform=transform\n",
    "    )\n",
    "    class_names = (\n",
    "        'plane', 'car', 'bird', 'cat', 'deer',\n",
    "        'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "    )\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=1, shuffle=False, num_workers=2\n",
    "    )\n",
    "    iterator = iter(loader)\n",
    "    images, _ = next(iterator)\n",
    "    shape = np.array(images.size()[1:])\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=False, num_workers=2\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_set, batch_size=batch_size, shuffle=False, num_workers=2\n",
    "    )\n",
    "    return SimpleNamespace(\n",
    "        train=train_loader, test=test_loader, classes=class_names, input_shape=shape\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:06<00:00, 25848220.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "loaders = cifar_10_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "outputs": [],
   "source": [
    "def show_sample(loader, classes):\n",
    "    iterator = iter(loader)\n",
    "    images, labels = next(iterator)\n",
    "    images = images / 2 + 0.5     # un-normalize\n",
    "    array = images.numpy()\n",
    "    n = array.shape[0]\n",
    "    plt.figure(figsize=(7, 2), tight_layout=True)\n",
    "    for k, a in enumerate(array):\n",
    "        plt.subplot(1, n, k + 1)\n",
    "        plt.imshow(np.transpose(a, (1, 2, 0)))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.xlabel(classes[labels[k]], fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_shape, convolutions=False):\n",
    "        super().__init__()\n",
    "        self.convolutions = convolutions\n",
    "        image_pixels = np.copy(input_shape[1:])\n",
    "        if self.convolutions:\n",
    "            self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "            for j in range(2):\n",
    "                image_pixels -= 4\n",
    "                image_pixels //= 2\n",
    "            m = 16 * prod(list(image_pixels))\n",
    "            self.fc1 = nn.Linear(m, 120)\n",
    "        else:\n",
    "            self.convolutions = False\n",
    "            input_size = prod(list(input_shape))\n",
    "            self.fc1 = nn.Linear(input_size, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.convolutions:\n",
    "            x = self.pool(fct.relu(self.conv1(x)))\n",
    "            x = self.pool(fct.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = fct.relu(self.fc1(x))\n",
    "        x = fct.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 3.1 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the purpose of the for loop is to iterate through the two convolution-pooling layers in the network. The loop essentially applies the dimension transformations of the convolution and pooling layers to the image. This is done by the two lines explained below.\n",
    "\n",
    "image_pixels -= 4: this line is subtracting 4 from the image size, specifically 4 from both the height and the width. For each convolutional layer, the kernel size is 5x5 which reduces 4 pixels, 2 from each side.\n",
    "\n",
    "image_pixels //= 2: this is line is dividing both the height and width of the image by 2, because each pooling layer is using 2x2 nonoverlapping windows to reduce the number of dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculations for convolutions = False:\n",
    "\n",
    "First Fully Connected Layer: \n",
    "\n",
    "Input size = $3*32*32 = 3072$. Thus, the number of parameters = $3072*120 + 120 = 368760$\n",
    "\n",
    "Second Fully Connected Layer:\n",
    "\n",
    "$84*120 + 84 =  850$ \n",
    "\n",
    "Third Fully Connected Layer: \n",
    "\n",
    "$84*120 + 84 = 10164$ \n",
    "\n",
    "#### Calculations for convolutions = True:\n",
    "\n",
    "First Fully Connected Layer: \n",
    "\n",
    "We start with an image size of 32x32. After the first convolution-pooling layer, this becomes 32x32 --> (32-4) x (32-4) --> 28x28 --> 28//2 x 28//2 --> 14 x 14. After the second convolution pooling layer, this becomes 14x14 --> (14-4)x(14-4) --> 10x10 --> (10//2)x(10//2) --> 5x5. Thus, our number of parameters = $(5*5*16*120)+120 = 48120$\n",
    "\n",
    "Second Fully Connected Layer:\n",
    "\n",
    "$84*120 + 120 = 10164$\n",
    "\n",
    "Third Fully Connected Layer: \n",
    "\n",
    "$84*10 + 10 = 850$ \n",
    "\n",
    "First Convolutional Layer: \n",
    "\n",
    "3 input channels, 6 output channels, kernel size is 5x5. This gives us $(3*6*5*5) + 6 = 456$\n",
    "\n",
    "Second Convolutional Layer: \n",
    "\n",
    "6 input channels, 16 output channels, kernel size is 5x5. This gives us $(6*16*5*5) + 16 = 2416$\n",
    "\n",
    "#### Total parameters \n",
    "\n",
    "For convolutions = False, summing up the values we get $368760+850+10164 = 379,774$\n",
    "\n",
    "For convolutions = True, summing up the values we get $48120+10164+850+456+2416 = 62,006$\n",
    "\n",
    "These 2 numbers corroborate the numbers given in the problem statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    model, loader, epochs,\n",
    "    start_epoch=0, learning_rate=0.001,\n",
    "    momentum=0.9, print_interval=2000\n",
    "):\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(), lr=learning_rate, momentum=momentum\n",
    "    )\n",
    "    fmt = '[{:2d}, {:5d}] risk over last {} mini-batches: {:.3f}'\n",
    "    for epoch in range(epochs):\n",
    "        running_risk = 0.\n",
    "        for i, batch in enumerate(loader, 0):\n",
    "            images, labels = batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = model(images)\n",
    "            risk = loss_function(predictions, labels)\n",
    "            risk.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_risk += risk.item()\n",
    "            if i % print_interval == print_interval - 1:\n",
    "                block_risk = running_risk / print_interval\n",
    "                ep = epoch + start_epoch\n",
    "                print(fmt.format(ep + 1, i + 1, print_interval, block_risk))\n",
    "                running_risk = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, set_name):\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            images, labels = batch\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print('{} accuracy {:.2f} percent'.format(set_name, accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "outputs": [],
   "source": [
    "def experiment(data_loaders, convolutions, epochs):\n",
    "    model = SimpleNet(data_loaders.input_shape, convolutions=convolutions)\n",
    "    print('training')\n",
    "    train(model, data_loaders.train, epochs=epochs)\n",
    "    print('evaluating')\n",
    "    training_accuracy = evaluate(model, data_loaders.train, 'training')\n",
    "    test_accuracy = evaluate(model, data_loaders.test, 'test')\n",
    "    return training_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "[ 1,  2000] risk over last 2000 mini-batches: 1.910\n",
      "[ 1,  4000] risk over last 2000 mini-batches: 1.735\n",
      "[ 1,  6000] risk over last 2000 mini-batches: 1.649\n",
      "[ 1,  8000] risk over last 2000 mini-batches: 1.607\n",
      "[ 1, 10000] risk over last 2000 mini-batches: 1.592\n",
      "[ 1, 12000] risk over last 2000 mini-batches: 1.571\n",
      "evaluating\n",
      "training accuracy 47.25 percent\n",
      "test accuracy 46.18 percent\n"
     ]
    }
   ],
   "source": [
    "wo_train_acc, wo_test_acc = experiment(loaders, False, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy of the model with convolution layers was 47.69%, and the test accuracy was 46.86%. For the model without convolution layers, the training accuracy was 47.25% and the test accuracy was 46.18%. Because of this, the model with convolutions gives better test accuracy by 0.68%, and generalizes better since its test accuracy is higher and its training accuracy is higher as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 3.4 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the model shows an overall tendency to overfit because as the epochs become larger, the training accuracy tends to increase while the validation accuracy tends to decrease. This means that the model is overfitting to the training data as we run more epochs and this overfitting is causing the validation accuracy to go down. \n",
    "\n",
    "For best test-time performance, I would stop after 10 epochs because the validation accuracy is at its highest, of around 62%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 3.5 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical accuracy of this predictor is 10%, if it ignores its input and predicts a class drawn uniformly at random from all 10 possible classes. \n",
    "\n",
    "The predictors in the previous problems all have a test/validation accuracy that is greater than 10%, so yes, they do perform better than the blind predictor. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
